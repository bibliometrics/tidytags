---
title: "Using tidytags with a conference hashtag"
output:
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using tidytags with a conference hashtag}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---




```r
#library(tidytags)
```

This vignette serves as an introduction to how to use many **tidytags** functions through the example of analyzing tweets associated with the 2019 annual convention of the [Association for Educational Communications & Technology](https://aect.org/) (AECT): `#aect19`, `#aect2019`, or `#aect19inspired`. The information in this vignette is available scattered throughout the R documentation for the **tidytags** package. This vignette conveniently brings it all together in one place.

## Considerations Related to Ethics, Data Privacy, and Human Subjects Research

Before working through this demonstration of the capabilities of **tidytags**, please take a few moments to reflect on ethical considerations related to social media research.


**{tidytags} should be used in strict accordance with Twitter's [developer terms](https://developer.twitter.com/en/developer-terms/more-on-restricted-use-cases).**

Although most Institutional Review Boards (IRBs) consider the Twitter data that {tidytags} analyzes to _not_ necessarily be human subjects research, there remain ethical considerations pertaining to the use of the {tidytags} package that should be discussed. 

Even if {tidytags} use is not for research purposes (or if an IRB determines that a study is not human subjects research), "the release of personally identifiable or sensitive data is potentially harmful," as noted in the [rOpenSci Packages guide](https://devguide.ropensci.org/policies.html#ethics-data-privacy-and-human-subjects-research). Therefore, although you _can_ collect Twitter data (and you _can_ use {tidytags} to analyze it),    we urge care and thoughtfulness regarding how you analyze the data and communicate the results. In short, please remember that most (if not all) of the data you collect may be about people---and [those people may not like the idea of their data being analyzed or included in research](https://journals.sagepub.com/doi/full/10.1177/2056305118763366). 

We recommend [the Association of Internet Researchers' (AoIR) resources related to conducting analyses in ethical ways](https://aoir.org/ethics/) when working with data about people. AoIR's [ethical guidelines](https://aoir.org/reports/ethics3.pdf) may be especially helpful for navigating tensions related to collecting, analyzing, and sharing social media data.

## Twitter Archiving Google Sheets

A core functionality of **tidytags** is collecting tweets continuously with a [Twitter Archiving Google Sheet](https://tags.hawksey.info/) (TAGS).

For help setting up your own TAGS tracker, see the "Getting started with tidytags" (`vignette("setup", package = "tidytags")`) vignette, **Key Task #1**.

## read_tags()

To simply view a TAGS archive in R, you can use `read_tags()`. Here, we've openly shared a [TAGS tracker](https://docs.google.com/spreadsheets/d/18clYlQeJOc6W5QRuSlJ6_v3snqKJImFhU42bRkM_OX8) that has been collecting tweets associated with the AECT 2019 since September 30, 2019. Notice that this TAGS tracker is collecting tweets containing three different hashtags: `#aect19`, `#aect2019`, or `#aect19inspired`. As of August 5, 2022, this tracker had collected **2,564 tweets**. This tracker is active through today, although these hashtags seems to be largely inactive in recent years.

**tidytags** allows you to work in R with tweets collected by a TAGS tracker. This is done with the [**googlesheets4** package](https://CRAN.R-project.org/package=googlesheets4).

One requirement for using {googlesheets4} is that your TAGS tracker has been "published to the web." To do this, with the TAGS page open in a web browser, go to `File >> Share >> Publish to the web`. The `Link` field should be 'Entire document' and the `Embed` field should be 'Web page.' If everything looks right, then click the `Publish` button. Next, click the `Share` button in the top right corner of the Google Sheets window, select `Get shareable link`, and set the permissions to 'Anyone with the link can view.' The input needed for the `tidytags::read_tags()` function is either the entire URL from the top of the web browser when opened to a TAGS tracker, or a Google Sheet identifier (i.e., the alphanumeric string following "https://docs.google.com/spreadsheets/d/" in the TAGS tracker's URL). Be sure to put quotations marks around the URL or sheet identifier when entering it into `read_tags()` function.

Again, if you're having trouble setting up or publishing the TAGS tracker, see the "Getting started with tidytags" vignette (`vignette("setup", package = "tidytags")`), **Key Task #1**.


```r
tags_url <- "18clYlQeJOc6W5QRuSlJ6_v3snqKJImFhU42bRkM_OX8"
example_df_all <- read_tags(tags_url)
dim(example_df_all)
#> [1] 2564   18
```

**Note that there are alternative ways to access TAGS files.** One way is to simply download a CSV file from Google Sheets. In Google Sheets, navigate to `File >> Download >> Comma-separated values (CSV)` to do so. Be sure to do so from the TAGS Archive page. Once this file is downloaded, you can read it into R like any other CSV.


```r
example_df_all <- readr::read_csv("my-downloaded-tags-file.csv")
```

## pull_tweet_data()

With a TAGS tracker archive imported into R, **tidytags** allows you to gather quite a bit more information related to the TAGS-collected tweets with the `pull_tweet_data()` function. This function builds off the [**rtweet** package](https://docs.ropensci.org/rtweet/) (via `rtweet::lookup_tweets()`) to query the Twitter API.

However, **to access the Twitter API, whether through rtweet or tidytags, you will need to apply for developers' access from Twitter**. You do this [through Twitter's developer website](https://developer.twitter.com/en/apply-for-access).

The **rtweet** documentation already contains a very thorough vignette, "Authentication with rtweet" (`vignette("auth", package = "rtweet")`), to guide you through the process of authenticating access to the Twitter API. We recommend the **app-based authentication** method that uses `auth <- rtweet::rtweet_app()`, described in the [Apps](https://docs.ropensci.org/rtweet/articles/auth.html#apps) section of the vignette.

For further help getting your own Twitter API keys, see the "Getting started with tidytags" vignette (`vignette("setup", package = "tidytags")`), specifically **Pain Point #2**.

Note that your dataset will often contain fewer rows after running `pull_tweet_data()`. This is because `rtweet::lookup_tweets()` is searching for tweet status IDs that are currently available. Any tweets that have been deleted or made "protected" (i.e., private) since TAGS first collected them are dropped from the dataset. Rather than view this as a limitation, we see this as an asset to help ensure our data better reflects the intentions of the accounts whose tweets we have collected (see [Fiesler & Proferes, 2018](https://journals.sagepub.com/doi/full/10.1177/2056305118763366)).

Here, we demonstrate two different ways of using `pull_tweet_data()`. The first method is to query the Twitter API with the tweet ID numbers from the `id_str` column returned by **rtweet**. However, a limitation of TAGS is that the numbers in this column are often corrupted because Google Sheets considers them very large numbers (instead of character strings) and rounds them by putting them into exponential form. The results of this first method are stored in the variable `example_after_rtweet_A` below. The second method pulls the tweet ID numbers from the tweet URLs. For example, the tweet with the URL `https://twitter.com/tweet__example/status/1176592704647716864` has a tweet ID of `1176592704647716864`. The results of this second method are stored in the variable `example_after_rtweet_B` below.


```r
app <- rtweet::rtweet_app(bearer_token = Sys.getenv("TWITTER_BEARER_TOKEN"))
rtweet::auth_as(app)

example_after_rtweet_A <- pull_tweet_data(id_vector = example_df_all$id_str)
example_after_rtweet_B <- pull_tweet_data(url_vector = example_df_all$status_url)
```

When this vignette was run on Aug 05 22, the TAGS tracker had collected 18 variables associated with 2564 tweets. The first method searching with `id_str` collected 43 variables associated with 2195 tweets. The second method using 'tidytags::get_char_tweet_ids()' collected 43 variables associated with 2195 tweets.

Notice how many more variables are in the dataset after using `pull_tweet_data()`, and how many more tweets are in the dataset when using the second method. We have found that in the process of storing and retrieving tweet IDs, the long string of numbers can sometimes be interpreted as an object of type double (i.e., numeric) and subsequently converted into scientific notation form. This loses the specific identifying use of the string of numerical digits. Therefore, we strongly recommend the second method, obtaining tweet IDs from the tweet URL, which is why we have included `get_char_tweet_ids()` as an internal function in the **tidytags** package.

The built-in default of `pull_tweet_data()` is to simply enter the dataframe retrieved from  `read_tags()` and implement the second method, retrieving metadata starting with tweet URLs. That is, `pull_tweet_data(read_tags(example_url))`. Take a quick look at the result, viewed with the `glimpse()` function from the **dplyr** package:


```r
example_after_rtweet <- pull_tweet_data(read_tags(tags_url))
dplyr::glimpse(example_after_rtweet)
#> Rows: 2,195
#> Columns: 43
#> $ created_at                    <dttm> 2020-04-19 15:22:23, 2020-03-01 15:00:41, 2020-02-…
#> $ id                            <dbl> 1.251954e+18, 1.234207e+18, 1.229405e+18, 1.225138e…
#> $ id_str                        <chr> "1251954312772812801", "1234206946732830720", "1229…
#> $ full_text                     <chr> "RT @RoutledgeEd: Congrats to authors Joseph Rene C…
#> $ truncated                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…
#> $ display_text_range            <dbl> 140, 140, 140, 140, 268, 140, 140, 140, 140, 140, 2…
#> $ entities                      <list> [[<data.frame[1 x 2]>], [<data.frame[1 x 2]>], [<d…
#> $ source                        <chr> "<a href=\"https://mobile.twitter.com\" rel=\"nofol…
#> $ in_reply_to_status_id         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ in_reply_to_status_id_str     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ in_reply_to_user_id           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ in_reply_to_user_id_str       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ in_reply_to_screen_name       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ geo                           <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
#> $ coordinates                   <list> [<data.frame[1 x 3]>], [<data.frame[1 x 3]>], [<da…
#> $ place                         <list> [<data.frame[1 x 3]>], [<data.frame[1 x 3]>], [<da…
#> $ contributors                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ is_quote_status               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…
#> $ retweet_count                 <int> 4, 28, 2, 2, 2, 9, 9, 9, 9, 9, 1, 9, 9, 9, 9, 9, 9,…
#> $ favorite_count                <int> 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 21,…
#> $ favorited                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…
#> $ retweeted                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…
#> $ possibly_sensitive            <lgl> NA, NA, NA, NA, FALSE, NA, NA, NA, NA, NA, FALSE, N…
#> $ lang                          <chr> "en", "en", "en", "en", "en", "en", "en", "en", "en…
#> $ retweeted_status              <list> [<data.frame[1 x 30]>], [<data.frame[1 x 30]>], [<…
#> $ quoted_status_id              <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1.218549e+1…
#> $ quoted_status_id_str          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, "1218548635…
#> $ quoted_status_permalink       <list> [<data.frame[1 x 3]>], [<data.frame[1 x 3]>], [<da…
#> $ quoted_status                 <list> [<data.frame[1 x 26]>], [<data.frame[1 x 26]>], [<…
#> $ text                          <chr> "RT @RoutledgeEd: Congrats to authors Joseph Rene C…
#> $ favorited_by                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ scopes                        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ display_text_width            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ quote_count                   <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ timestamp_ms                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ reply_count                   <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ filter_level                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ metadata                      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ query                         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ withheld_scope                <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ withheld_copyright            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ withheld_in_countries         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
#> $ possibly_sensitive_appealable <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
```

At this point, the purpose of **tidytags** should be restated. TAGS tweet trackers are easily set up and maintained, and they do an excellent job passively collecting tweets over time. For instance, the example TAGS tracker we demo here has collected thousands of tweets related to the AECT 2019 annual convention since September 30, 2019. In contrast, running this query now using `rtweet::search_tweets()` is limited by Twitter's API, meaning that an **rtweet** search can only go back in time 6-9 days, and is limited to returning at most 18,000 tweets per query. So, if you are interested in tweets about AECT 2019, today you could get almost no meaningful data using **rtweet** alone.


```r
rtweet_today <-
  rtweet::search_tweets("#aect19 OR #aect2019 OR #aect19inspired")
```

You can see that an **rtweet** search for #AECT19 tweets run today returns 0tweets.

In sum, although a TAGS tracker is great for easily collecting tweets over time (breadth), it lacks depth in terms of metadata related to the gathered tweets. Specifically, TAGS returns information on at most 18 variables; in contrast, **rtweet** returns information on 43 variables. Thus, **tidytags** brings together the breadth of TAGS with the depth of **rtweet**.

## lookup_many_tweets()

The Twitter API only allows the looking up of 90,000 tweet IDs at a time, a rate limit which resets after 15 minutes. Hence `rtweet::lookup_tweets()` will only return results for the first 90,000 tweet IDs in your dataset. The function `tidytags::lookup_many_tweets()` will automatically divide your dataset into batches of 90,000 tweets, looking up one batch per 15 minutes until finished. Note that `lookup_many_tweets()` also works for datasets with fewer than 90,000 tweets.

Because our AECT 2019 examples includes fewer than 90,000 tweets (and because `lookup_many_tweets()` involves waiting for 15 minutes between batches), we do not include an example here. However, this function can be used in the same way as `pull_tweet_data()`.

## process_tweets()

After `pull_tweet_data()` is used to collect additional information from TAGS tweet IDs (in this case, the `example_after_rtweet` dataframe), the **tidytags** function `process_tweets()` can be used to add user information about the tweeters through `rtweet::users_data()`. `process_tweets()` also calculates additional attributes and adds these to the dataframe as new columns. Specifically, five new variables are added: mentions_count, hashtags_count, urls_count, tweet_type, and is_self_reply. This results in 71 variables associated with the collected tweets.
























